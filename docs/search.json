[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Sanjay M",
    "section": "",
    "text": "Forest cover classification\nSimple multi-output regression model for predicting canopy height and canopy cover\n\n\n   Cloud cover removal using GenAI\nPix2Pix model based cloud cover removal technique for Satellite images\n\n\n\n\n\n\n   High Carbon Stock approach (HCS)\nAboveground biomass estimation using Google deepmind’s embedding dataset\n\n\n   Deforestation detection using Unet\nUnet based segmentation of sentinel 1 & sentinel 2 images\n\n\n\n\n\n\n   Globaleco - Python package\nA simple python package for seamless downloading and visualizing GEDIL4A data\n\n\n   Bird migration visualization\nPydeck based visualization for migration path of three birds"
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Sanjay M",
    "section": "",
    "text": "Forest cover classification\nSimple multi-output regression model for predicting canopy height and canopy cover\n\n\n   Cloud cover removal using GenAI\nPix2Pix model based cloud cover removal technique for Satellite images\n\n\n\n\n\n\n   High Carbon Stock approach (HCS)\nAboveground biomass estimation using Google deepmind’s embedding dataset\n\n\n   Deforestation detection using Unet\nUnet based segmentation of sentinel 1 & sentinel 2 images\n\n\n\n\n\n\n   Globaleco - Python package\nA simple python package for seamless downloading and visualizing GEDIL4A data\n\n\n   Bird migration visualization\nPydeck based visualization for migration path of three birds"
  },
  {
    "objectID": "hcs/index.html",
    "href": "hcs/index.html",
    "title": "High Carbon Stock approach for Palm oil cultivated region, Malaysia",
    "section": "",
    "text": "One of the most striking challenges was the dominance of palm oil cultivation and its implications for forest conservation. Palm oil expansion often encroaches on biodiverse, carbon-rich forests, and distinguishing which lands should be prioritized for conservation is a central goal of the High Carbon Stock (HCS) Approach. I wanted to build a workflow that would not only estimate carbon stock but also make the results transparent and accessible through a visualization tool.\nThe first step was to estimate aboveground biomass (AGB) across the region. For this, I turned to GEDI L4A data, which provides high-resolution lidar-based biomass and canopy structure measurements. GEDI footprints were sparse, but they offered an essential ground-truth signal. To scale beyond the footprint level, I combined them with Google DeepMind’s embedding datasets, which provide rich feature representations of satellite imagery across multiple modalities. By training a regression model on this combination, I was able to generalize GEDI’s biomass estimates across the wider Sabah landscape.\nOnce AGB was mapped, the next step was to convert it into carbon stock estimates. Using standard conversion factors (roughly half of dry biomass is carbon), I produced wall-to-wall carbon stock maps for the region. These maps revealed the expected patterns: high carbon storage in intact forests of Sabah’s interior, and much lower values across oil palm estates and degraded areas.\nWith carbon stock maps in hand, I applied the HCS Approach classification framework. This methodology divides landscapes into different classes based on carbon thresholds — from high-density forests that should be protected, to low-carbon scrublands and already converted lands. Using the thresholds, I reclassified the continuous carbon stock data into discrete HCS classes, effectively creating a decision-support layer for conservation planning.\nThe final part of the project was to make this analysis accessible to public. I built a Google Earth Engine (GEE) application where users can explore Sabah’s landscape interactively. The app allows zooming into specific regions, overlaying HCS classes on satellite basemaps, and toggling between AGB, carbon stock layers, satellite embeddingd and GEDI L4A footprints. With this tool, conservation organizations could instantly see which areas qualify as high carbon stock forests and where palm oil expansion would pose the greatest environmental risks."
  },
  {
    "objectID": "hcs/index.html#high-carbon-stock-map-of-sabah-malaysia",
    "href": "hcs/index.html#high-carbon-stock-map-of-sabah-malaysia",
    "title": "High Carbon Stock approach for Palm oil cultivated region, Malaysia",
    "section": "High carbon stock map of Sabah, Malaysia",
    "text": "High carbon stock map of Sabah, Malaysia\n\n\n\nHCS\n\n\nThis project showed how combining state-of-the-art data sources like GEDI and DeepMind embeddings with cloud-native geospatial platforms can bring transparency to land-use decisions. By grounding the work in the HCS Approach, it also provided a pathway for balancing palm oil development with forest conservation — ensuring that some of the world’s richest carbon and biodiversity reserves are protected for the future.\nCode : https://github.com/Sanjay-geospatial/High-carbon-stock-approach"
  },
  {
    "objectID": "forest_cover/index.html",
    "href": "forest_cover/index.html",
    "title": "Cloud native forest cover classification using deep learning",
    "section": "",
    "text": "When I began working on forest cover classification, my primary motivation was to capture the complexity of forest ecosystems beyond simple “forest / non-forest” masks. I wanted to design a system that could estimate multiple forest attributes simultaneously — canopy height and cover cover — and do so in a scalable way using openly available satellite data. Instead of relying on local datasets or offline processing, I decided to build the entire pipeline in the cloud, leveraging STAC APIs and NASA’s Earthdata to pull harmonized datasets directly into the workflow.\nThe data foundation was critical. I selected three complementary sources: GEDI LiDAR footprints to provide precise canopy structure information, ALOS PALSAR radar backscatter to capture vegetation texture and moisture sensitivity, and Landsat multispectral imagery to contribute spectral indices like NDVI, NDWI etc. Together, these offered a robust multi-sensor perspective of forest conditions. Using Python’s robust modular programming, I developed a pipeline for data access and data preprocessing with very few classes which allowed me to query STAC client datasets efficiently, filter them temporally and spatially, and prepare training samples without downloading terabytes of raw files.\nModel design was another challenge. Since my objective was to predict several forest parameters at once, I moved away from traditional single-output classification models. Instead, I implemented a multi-output regression model in PyTorch. The architecture was very simple with few hidden layers and multiple regression heads at the output stage — each tuned for a specific forest attribute. This design allowed the model to learn shared representations of forest structure while still maintaining task-specific specialization.\nTraining such a model required careful orchestration. I used cloud GPUs and containerized the workflow so that it could run reproducibly. GEDI footprints served as ground truth labels for canopy height and cover cover. Loss functions were a weighted combination of mean squared error for each output, allowing me to balance accuracy across tasks.\nOnce trained, the model was evaluated on unseen forest regions. Results showed that integrating radar, LiDAR, and optical data dramatically improved predictive power compared to any single sensor with an r square value of 0.5. The model generalized well, producing consistent canopy height and canopy cover estimates . And the final forest cover classification was carried based on the following criteria."
  },
  {
    "objectID": "forest_cover/index.html#predicted-outputs-canopy-height-and-canopy-cover",
    "href": "forest_cover/index.html#predicted-outputs-canopy-height-and-canopy-cover",
    "title": "Cloud native forest cover classification using deep learning",
    "section": "Predicted outputs : Canopy height and Canopy cover",
    "text": "Predicted outputs : Canopy height and Canopy cover\n\n\n\nForest Cover\n\n\n\n\n\nTree Cover\nCanopy Height\nClass\n\n\n\n\n&gt; 0.7\n&gt; 20m\nPrimary Forest\n\n\n0.4–0.7\n5–20m\nSecondary Forest\n\n\n&lt; 0.4\n&lt; 5m\nOthers\n\n\n\nOne of the most rewarding moments was visualizing continuous forest structure maps directly in the cloud — no need for heavy downloads, just tiled outputs accessible through a simple pipeline.\nUltimately, this project demonstrated how modern geospatial AI workflows can combine cloud computing, open APIs, and deep learning to produce actionable environmental intelligence. By designing a multi-output regression approach, I moved beyond simple forest classification toward richer forest characterization. And by building it entirely on cloud-native tools, the system remains scalable and reproducible, paving the way for monitoring forest dynamics at regional to global scales. Thanks for reading.\nCode : https://github.com/Sanjay-geospatial/Forest-cover-classification"
  },
  {
    "objectID": "cloud_cover/index.html",
    "href": "cloud_cover/index.html",
    "title": "Cloud cover removal app using the power of GenAI",
    "section": "",
    "text": "cloud_app\n\n\nThe idea for this project came from one of the most persistent challenges in satellite remote sensing: cloud cover. Optical satellites like Landsat and Sentinel-2 are invaluable for Earth observation, but a significant fraction of their imagery is obscured by clouds. Traditional methods of cloud masking often discard large portions of data, leaving gaps in analysis. I wanted to see whether generative AI could reconstruct the obscured pixels and provide cloud-free imagery that was still useful for downstream applications.\nI chose the Pix2Pix architecture, a conditional GAN model originally designed for image-to-image translation. The intuition was simple: treat cloud-covered satellite images as the “input domain” and their cloud-free counterparts as the “target domain.” By training the model on paired datasets where both cloudy and cloud-free versions of the same location existed, the generator could learn how to “hallucinate” realistic surface features beneath the clouds, while the discriminator kept the outputs sharp and plausible. I built the model using TensorFlow, customizing the loss functions to balance adversarial training with pixel-wise reconstruction fidelity.\nCollecting and preparing training data was an important step. Using STAC API, I compiled pairs of cloudy and near-simultaneous cloud-free sentinel 2 images. Due to limitation of computing resources, the model was trained only with a single epoch. However, the results were quite satisfactory.\nBut building the model was only half the journey. I wanted people beyond the research community to actually use it. For this, I deployed the trained Pix2Pix model on Hugging Face Spaces, taking advantage of their model hosting and inference API. This made the model easily accessibl, removing the need for anyone to set up TensorFlow locally.\nTo create a more interactive experience, I built a Streamlit web app where users could upload their own cloudy satellite image patches and instantly view reconstructed outputs. The interface was intentionally simple: upload an image, click a button and see the before-and-after comparison side by side. The backend connected seamlessly to the Hugging Face model endpoint, ensuring scalability and responsiveness.\nThe project quickly gained attention as an example of how Generative AI can be harnessed for real-world geospatial problems. While I emphasize that the outputs should not replace original observations in critical workflows (since generative reconstructions may introduce artifacts), the tool demonstrated how cloud cover no longer had to be a dead end.\nCode : https://github.com/Sanjay-geospatial/Pix2pix-Cloud-free-image-generation"
  },
  {
    "objectID": "birds/index.html",
    "href": "birds/index.html",
    "title": "3D visualization of bird migration paths across continents",
    "section": "",
    "text": "migration\n\n\nOne of the most fascinating ecological phenomena to visualize is the seasonal migration of birds. For this project, I wanted to create an interactive way to see how bird species move across continents over time, transforming tabular movement records into a dynamic 3D globe simulation. The goal was to map static migration routes interactively.\nTo achieve this, I used Pydeck, a Python wrapper for Deck.gl, which is well-suited for large-scale geospatial visualization on a 3D globe. I ingested migration datasets containing latitude, longitude, and time information for different species, then processed them into trajectory segments. Pydeck’s seamless functions allowed me to represent migration paths as animated arcs that traced bird movements across oceans and continents in near-realistic globe.\nThe final simulation presented a rotatable 3D globe, where users could zoom, tilt, and watch as migratory routes unfolded. Color coding distinguished species. The visualization not only made the complexity of migration easier to grasp, but also demonstrated how modern geospatial tools like Pydeck can bring ecological data to life in a way that is both scientifically insightful and visually engaging.\nCode : https://github.com/Sanjay-geospatial/Pydeck_birds_migration"
  },
  {
    "objectID": "deforestation/index.html",
    "href": "deforestation/index.html",
    "title": "Deforestation detection using using multisource satellite data with Unet",
    "section": "",
    "text": "The goal was to explore how deep learning could help track one of the most pressing environmental crises of our time — deforestation in the Amazon rainforest. The Amazon is vast, dynamic, and often obscured by clouds, making reliable monitoring a major technical challenge. I decided to design a deep learning pipeline that combined radar and optical satellite data, giving the model the best possible chance to “see through” different conditions and detect forest loss.\nFor the dataset, I relied on a publicly available Amazon deforestation Kaggle dataset, which contained labeled samples of forested and deforested areas. From the satellite side, I used Sentinel-1 radar (VV, VH polarizations) to provide structural information about vegetation, and Sentinel-2 optical bands (Blue, Green, Red, NIR) to capture spectral patterns associated with vegetation health and land cover change. By stacking these channels, the model could learn complementary information from both radar and optical perspectives.\nI chose a U-Net architecture for segmentation, as it is well-suited for pixel-level classification in remote sensing. U-Net’s encoder-decoder design allowed the model to capture global context while preserving fine-grained spatial details through skip connections. The task was formulated as binary segmentation: classifying each pixel as either forest or deforested. For training, I used Binary Cross-Entropy (BCE) loss, which worked well for this two-class problem, and accuracy as a straightforward evaluation metric.\nTraining the model required careful balancing of the dataset, since deforested areas were relatively small compared to intact forest. After several epochs, the model began to converge strongly. The final results were promising: a binary cross-entropy loss of 0.16 and an accuracy of 0.94 on the validation set. This meant the U-Net was able to correctly classify most pixels, capturing the complex boundary between forest and cleared land with good reliability.\nCode : https://github.com/Sanjay-geospatial/Deforestation-detection-using-Unet"
  },
  {
    "objectID": "globaleco/index.html",
    "href": "globaleco/index.html",
    "title": "A python package for easy access and visualization of GEDI L4A data",
    "section": "",
    "text": "gedi\n\n\nThis project started from a simple but recurring need: downloading and visualizing GEDI L4A data efficiently. While NASA Earthdata provides GEDI products, the workflow for accessing, downloading, and exploring the data is often fragmented and repetitive. I wanted to streamline this process into a lightweight Python package that anyone working with GEDI could quickly adopt.\nI designed the package around modular programming principles, with each component dedicated to a specific task. At its core, the package introduces a GEDI class that helps users discover and retrieve links to GEDI L4A files. Its key method, get_links, queries the GEDI archive and returns all relevant file links neatly organized in a Pandas DataFrame. This step makes the discovery process transparent and allows users to filter or inspect links before downloading.\nThe GEDI class also comes with a download method, which takes the links from the DataFrame and automatically retrieves the GEDI data to a local directory. This method handles the data pipeline seamlessly, so users don’t have to worry about constructing URLs or managing download scripts.\nFor working with the downloaded data, I developed a second class: GEDIReader. This class is responsible for interpreting and interacting with the actual GEDI files. Its main method, get_gedi, reads the contents of the downloaded files into a Python-friendly format, enabling further analysis.\nFinally, I added a visualize method within GEDIReader to provide quick-look visualizations of the GEDI footprints. With a single command, users can see where the laser footprints fall on a map, giving immediate spatial context to the data they just downloaded. This visualization helps bridge the gap between raw data files and actionable insights, making it easier for users to decide how to incorporate GEDI into their workflows.\nThe result is a simple, modular Python package that lowers the barrier for working with GEDI L4A data. By encapsulating the workflow into discover → download → read → visualize, the package saves time and provides an intuitive entry point for researchers, students, or practitioners interested in forest structure and biomass studies. It also demonstrates how modular, class-based design can make scientific software both clean and practical.\nCode : https://github.com/Sanjay-geospatial/GEDI—globaleco"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sanjay M",
    "section": "",
    "text": "👋 Hi, I'm Sanjay\nWelcome to my portfolio website!\nI’m a GIS Specialist and an aspiring Geospatial Data Scientist with expertise in Forestry, GIS, and AI for sustainability.\nCurrently, I am helping my company track supply chains for EUDR compliance, ensuring transparency and reducing deforestation."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Sanjay M",
    "section": "🧑‍💻 About Me",
    "text": "🧑‍💻 About Me\n\n🌍 Passionate about applying geospatial AI for climate, forestry, and agriculture\n\n🔭 Skilled in GIS, Deep Learning, and Earth Observation Data\n\n📊 Experienced with Python, PyTorch, STAC API, Google Earth Engine and PostGIS\n\n🎯 Focus areas: Deforestation monitoring, Carbon Stock Estimation, and Forest Cover Analytics"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sanjay M",
    "section": "🎓 Education",
    "text": "🎓 Education\nM.Sc. (Hons.) Forestry – Keladi Shivappa Nayaka University of Agricultural & Horticultural Sciences (2022–2024)\nB.Sc. (Hons.) Forestry – University of Agricultural & Horticultural Sciences (2017–2021)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Sanjay M",
    "section": "📬 Contact",
    "text": "📬 Contact\n\n📧 Email: sanjaymanjappa25@gmail.com\n\n📞 Phone : +91 9900797596"
  }
]